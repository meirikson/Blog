[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "A VERY GOOD Data Science Blog",
    "section": "",
    "text": "The Go AI that Defeated Humanity\n\n\n\nAI\n\nML\n\nReinforcement Learning\n\nGo\n\n\n\n\n\n\n\n\n\nJan 15, 2026\n\n\nMichael Eirikson\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "This blog is written by Michael Eirikson. The content is mainly topics in data science, programming and statistics."
  },
  {
    "objectID": "posts/alpha-go/index.html",
    "href": "posts/alpha-go/index.html",
    "title": "The Go AI that Defeated Humanity",
    "section": "",
    "text": "In 2016 a team from Google DeepMind revealed AlphaGo, the most powerful Go AI ever created. Initially top Go players took it for granted they could be this new AI. Less than a year before the release of AlphaGo there were experts who though AI beating top Go players was at least a decade away if not completely impossible.\nTo prove the strength of AlphaGo, the team from DeepMind challenged a top Go player named Lee Sedol to a best of five match. Lee Sedol had been the number one raked Go player worldwide from 2007 to 2011 and remained a top level pro player until he retired in 2019.\nIt didn’t take long for Sedol to realise how strong AlphaGo really was. AlphaGo won the first three games. After losing the third game Sedol apologized for his losses and stated “I misjudged the capabilities of AlphaGo and felt powerless.” Sedol did manage to win game four but then lost the final round. Sedol’s loss completely stunned the Go community, who universally assumed Sedol would win easily.\n\n\n\nLee Sedol playing Against AlphaGo"
  },
  {
    "objectID": "posts/alpha-go/index.html#this-history",
    "href": "posts/alpha-go/index.html#this-history",
    "title": "The Go AI that Defeated Humanity",
    "section": "",
    "text": "In 2016 a team from Google DeepMind revealed AlphaGo, an AI capable of beating the worlds best Go players. Less than a year earlier experts were saying this was at least a decade away if not completely impossible. When the AI was shown for the first time, Go players around the world were shocked at the strength of the AI, while AI developers and enthusiasts were fascinated by the techniques used to reach this achievement.\nGo is a game played between two players on a 19x19 grid. The players take turns placing one stone with the goal of claiming the most territory. The rules set is simple but lead to an extremely complex and intricate game.\nBefore AlphaGo the best Go AI’s were based on a technique called Monte Carlo Tree Search (MCTS). In MCTS on each turn the AI performs thousands of random playouts. In a playout the AI randomly simulates the rest of the game. While doing these playouts the AI keeps track of the number of wins and losses that proceed from every position that it explores. While simulating the playout it balances a trade off between exploring lots of possible different moves and exploiting moves that are performing well. With a sufficient number of playouts this type of AI was able to play at a professional level on smaller 9x9 boards and play at a strong amateur level on full 19x19 boards.\nIn 2016 the Google DeepMind team challenged a top Go player named Lee Sedol to a best of five match. Lee Sedol was the number one raked Go player worldwide from 2007 to 2011 and remained a top level pro player till he retired in 2019. After the first three games AlphaGo was ahead 3-0. After losing the third game Sedol apologised for his losses and stated “I misjudged the capabilities of AlphaGo and felt powerless.” Sedol won game four then lost the final round. Sedol’s loss stunned the Go community, where an easy win had been expected.\nAlphaGo uses a version of MCTS. In AlphaGo, instead of performing a playout all the way till the game ends, it uses a model to predicted if a given position is good or bad and records this value in place of the result of the game. It also uses this model to chose the best moves to explore.\nThe model works by first taking the current position and recent history. Then this data is fed though 40 processing layers. These layers work together to process the board position using a technique reminiscent of how a visual cortex processes information. In each layer patterns are detected and a representation of those patterns is given as output. In this way each layer builds on the patterns of the previous layer. Initial layers might detect and process simples patters like two adjacent stone while later layers would process extremely complex arrangements of stones. After these 40 layers a bit more processing is done to produce two predictions: how good or bad the position is, and a list of the best possible moves to further explore.\nThis was the technique the allowed AI to finally beat the worlds best Go players."
  },
  {
    "objectID": "posts/alpha-go/index.html#the-history",
    "href": "posts/alpha-go/index.html#the-history",
    "title": "The Go AI that Defeated Humanity",
    "section": "",
    "text": "In 2016 a team from Google DeepMind revealed AlphaGo, an AI capable of beating the worlds best Go players. Less than a year earlier experts were saying this was at least a decade away if not completely impossible. When the AI was shown for the first time, Go players around the world were shocked at the strength of the AI, while AI developers and enthusiasts were fascinated by the techniques used to reach this achievement.\nIn 2016 the Google DeepMind team challenged a top Go player named Lee Sedol to a best of five match. Lee Sedol was the number one raked Go player worldwide from 2007 to 2011 and remained a top level pro player till he retired in 2019. After the first three games AlphaGo was ahead 3-0. After losing the third game Sedol apologised for his losses and stated “I misjudged the capabilities of AlphaGo and felt powerless.” Sedol won game four then lost the final round. Sedol’s loss stunned the Go community, where an easy win had been expected."
  },
  {
    "objectID": "posts/alpha-go/index.html#what-is-go-anyways",
    "href": "posts/alpha-go/index.html#what-is-go-anyways",
    "title": "The Go AI that Defeated Humanity",
    "section": "What is Go Anyways?",
    "text": "What is Go Anyways?\nGo is a game played between two players on a 19x19 grid. The players take turns placing one stone with the goal of claiming the most territory. The game ends when neither player wishes to make another move. The rules set is simple but lead to an extremely complex and intricate game. Games tend to take about 150-200 moves.\nA lot of the complexity of Go comes from the vast number of possible games that can be played. There are about \\(2 \\times 10^{170}\\) legal positions (that is a 2 followed by 170 zeros). For context this is roughly the number of particles in the visible universe squared. But that is just the number of legal positions, estimated for the total number of actual playable gamse games start around \\(2 \\times 10^{700}\\)! This is a difficult thing to calculate and different methods yield a wide range of results. The point is, the space of all possible games is massively large on a scale that is hard to even think about.\n\n\n\nA typical go game close to the end"
  },
  {
    "objectID": "posts/alpha-go/index.html#before-alphago",
    "href": "posts/alpha-go/index.html#before-alphago",
    "title": "The Go AI that Defeated Humanity",
    "section": "Before AlphaGo",
    "text": "Before AlphaGo\nBefore AlphaGo the best Go AI’s were based on a technique called Monte Carlo Tree Search (MCTS).\nIn MCTS on each turn the AI performs thousands of random playouts. In a playout the AI starts with the current position and randomly simulates the rest of the game. While doing these playouts the AI does some simple bookkeeping to track of the number of wins and losses that proceed from every position that it explores and also the number of time a position has been visited. This bookkeeping along with some simple but clever math allows the AI to balances a trade off between exploring lots of possible different moves and exploiting moves that are performing well.\nWith a sufficient number of playouts this type of AI was able to play at a professional level on smaller 9x9 boards and play at a strong amateur level on full 19x19 boards. But these models were very far from beating top Go players on full boards.\nThe main reason this method wasn’t able to surpass the worlds best Go players is because of the vast number of games that need to be searched. MCTS methods search through possible games randomly, they don’t have a mechanism that help them select better moves. Because of this inefficient search mechanism and the huge number of moves to explore, MCTS will never be able to reliably find the best possible moves to play.\n\n\n\nThe four steps of Monte Carlo Tree Seach"
  },
  {
    "objectID": "posts/alpha-go/index.html#how-alphago-actually-works",
    "href": "posts/alpha-go/index.html#how-alphago-actually-works",
    "title": "The Go AI that Defeated Humanity",
    "section": "How AlphaGo Actually Works",
    "text": "How AlphaGo Actually Works\nAlphaGo uses a version of MCTS. In AlphaGo, instead of performing a playout all the way till the game ends, it uses a model to predicted if a given position is good or bad and records this value in place of the result of the game. It also uses this model to chose the best moves to explore.\nThe model works by first taking the current position and recent history. Then this data is fed though 40 processing layers. These layers work together to process the board position using a technique reminiscent of how a visual cortex processes information. In each layer patterns are detected and a representation of those patterns is given as output. In this way each layer builds on the patterns of the previous layer. Initial layers might detect and process simples patters like two adjacent stone while later layers would process extremely complex arrangements of stones. After these 40 layers a bit more processing is done to produce two predictions: how good or bad the position is, and a list of the best possible moves to further explore.\n\n\n\nhttps://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0\n\n\nThis was the technique the allowed AI to finally beat the worlds best Go players."
  },
  {
    "objectID": "posts/alpha-go/index.html#the-story",
    "href": "posts/alpha-go/index.html#the-story",
    "title": "The Go AI that Defeated Humanity",
    "section": "",
    "text": "In 2016 a team from Google DeepMind revealed AlphaGo, the most powerful Go AI ever created. Initially top Go players took it for granted they could be this new AI. Less than a year before the release of AlphaGo there were experts who though AI beating top Go players was at least a decade away if not completely impossible.\nTo prove the strength of AlphaGo, the team from DeepMind challenged a top Go player named Lee Sedol to a best of five match. Lee Sedol had been the number one raked Go player worldwide from 2007 to 2011 and remained a top level pro player until he retired in 2019.\nIt didn’t take long for Sedol to realise how strong AlphaGo really was. AlphaGo won the first three games. After losing the third game Sedol apologized for his losses and stated “I misjudged the capabilities of AlphaGo and felt powerless.” Sedol did manage to win game four but then lost the final round. Sedol’s loss completely stunned the Go community, who universally assumed Sedol would win easily.\n\n\n\nLee Sedol playing Against AlphaGo"
  },
  {
    "objectID": "posts/alpha-go/index.html#so-what-does-alphago-do",
    "href": "posts/alpha-go/index.html#so-what-does-alphago-do",
    "title": "The Go AI that Defeated Humanity",
    "section": "So What does AlphaGo Do?",
    "text": "So What does AlphaGo Do?\nAlphaGo solves this problem by using something called a convolutional neural network (CNN) to evaluate how good a given position is for each player and make predictions about what the best possible moves are. More on the CNN later.\nSo AlphaGo makes uses an adaptation of MTCS. The difference is that while the AI is performing playouts for the game it uses the CNN model to do two things. First instead of searching randomly it uses the model to predict good possible moves and simulates games uses those. Second, when it comes to a new position it hasn’t visited yet, instead of simulating all the way till the game ends. It just passes this new position into the CNN which predicts how good of a position it is. This prediction is then recorded in place of recording wins and losses. So AlphaGo is able to search much more efficiently, it doesn’t need to simulated a game all the way to the end and it chooses generally good moves to explore.\nOkay great, but how does the CNN predict good moves and evaluate how good the position is? Isn’t that really just the problem we’re trying to solve in the first place? Yes that’s true. We’ll get into how the CNN makes these prediction in a moment. But first, the reason we are still doing MCTS along with this CNN is to refine the prediction from the CNN. It is the difference between a player looking at one position and having some instinct about how good it is and what they should do versus looking many moves ahead along many paths that they have some intuition about. Thinking ahead in this way will always result in much stronger moves.\n\n\n\nhttps://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0"
  },
  {
    "objectID": "posts/alpha-go/index.html#what-does-the-cnn-do",
    "href": "posts/alpha-go/index.html#what-does-the-cnn-do",
    "title": "The Go AI that Defeated Humanity",
    "section": "What Does the CNN Do?",
    "text": "What Does the CNN Do?\nThe model works by first taking the current position and recent history. Then this data is fed though 40 processing layers. These layers work together to process the board position using a technique reminiscent of how a visual cortex processes information. In each layer patterns are detected and a representation of those patterns is given as output. In this way each layer builds on the patterns of the previous layer. Initial layers might detect and process simples patters like two adjacent stone while later layers would process extremely complex arrangements of stones. After these 40 layers a bit more processing is done to produce two predictions: how good or bad the position is, and a list of the best possible moves to further explore.\n\n\n\nhttps://medium.com/applied-data-science/alphago-zero-explained-in-one-diagram-365f5abf67e0\n\n\nThis was the technique the allowed AI to finally beat the worlds best Go players."
  },
  {
    "objectID": "posts/alpha-go/index.html#the-convolutional-neural-network",
    "href": "posts/alpha-go/index.html#the-convolutional-neural-network",
    "title": "The Go AI that Defeated Humanity",
    "section": "The Convolutional Neural Network",
    "text": "The Convolutional Neural Network\nThe CNN is made up of 40 processing layers. Each of these layers has hundreds of thousands of tunable parameters. It is the tuning of these parameters that will allow the CNN model to “learn”.\nTo make predictions the model is given the current position and recent history. Then this data is fed though 40 processing layers. These layers work together to process the board position using a technique reminiscent of how a visual cortex processes information. In each layer patterns are detected and a representation of those patterns is given as output. In this way each layer builds on the patterns of the previous layer. Initial layers might detect and process simple patters like two adjacent stone while later layers would process extremely complex arrangements of stones. After these 40 layers a bit more processing is done to produce two predictions: how good or bad the position is, and a list of the best possible moves to further explore.\nLearning is accomplished by playing AlphaGo against itself and recording the moves and the outcome. Then through some clever math not covered here: the tunable parameters are changed in a small way that will cause moves that led to a win to be predicted more often and moves that led to a loss to be predicted less often. Also the CNNs prediction about how good the position is, is compared to the outcome of the game. These tunable parameters are also given a small change to make this prediction more accurate. Then by playing millions of games and making millions of small changes to these parameters the model slowly becomes better and better.\nThe end result is a model that is much better at guiding MCTS then its normal random search, and is much better at giving prediction on how good a position is than randomly playing all they way to the end.\nThis is how AlphaGo defeated the worlds best Go players."
  }
]